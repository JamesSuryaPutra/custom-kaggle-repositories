{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":116995,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":98328,"modelId":121954}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"# Install environment packages first\n!pip install -q -U immutabledict sentencepiece\n!pip install -q -U sacremoses transformers\n!git clone https://github.com/google/gemma_pytorch.git\n!mkdir /kaggle/working/gemma/\n!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/","metadata":{"execution":{"iopub.status.busy":"2024-10-11T05:20:35.968628Z","iopub.execute_input":"2024-10-11T05:20:35.968926Z","iopub.status.idle":"2024-10-11T05:21:21.129593Z","shell.execute_reply.started":"2024-10-11T05:20:35.968893Z","shell.execute_reply":"2024-10-11T05:21:21.128303Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'gemma_pytorch'...\nremote: Enumerating objects: 239, done.\u001b[K\nremote: Counting objects: 100% (123/123), done.\u001b[K\nremote: Compressing objects: 100% (70/70), done.\u001b[K\nremote: Total 239 (delta 87), reused 58 (delta 53), pack-reused 116 (from 1)\u001b[K\nReceiving objects: 100% (239/239), 2.18 MiB | 19.54 MiB/s, done.\nResolving deltas: 100% (136/136), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import primary function libraries\nimport contextlib\nimport gemma\nimport os\nimport sys\nimport torch\nimport transformers\n\n# Import secondary function libraries\nfrom transformers import MarianMTModel, MarianTokenizer\nfrom gemma.config import GemmaConfig, get_model_config\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-10-11T05:21:27.630248Z","iopub.execute_input":"2024-10-11T05:21:27.630666Z","iopub.status.idle":"2024-10-11T05:21:34.311635Z","shell.execute_reply.started":"2024-10-11T05:21:27.630626Z","shell.execute_reply":"2024-10-11T05:21:34.310847Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Define functional preferences","metadata":{}},{"cell_type":"code","source":"# Model variant and machine classification\nvariant = \"2b-v2\"\nmachine_type = \"cuda\"\nmodel_path = '/kaggle/input/gemma-2-2b-jpn-it/pytorch/gemma-2-2b-jpn-it/1/'\nweights_file = os.path.join(model_path, \"model.ckpt\")\n\n# Set the default Tensor type\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\n# Assign the model configuration along with the tokenizer\nmodel_config = get_model_config(variant)\nmodel_config.tokenizer = os.path.join(model_path, \"tokenizer.model\")\n\n# Set the device, e.g. GPU or CPU\ndevice = torch.device(machine_type)\n\n# Load the model\nwith _set_default_tensor_type(model_config.get_dtype()):\n    model = GemmaForCausalLM(model_config)\n    model.load_weights(weights_file)\n    model = model.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2024-10-11T05:21:39.720294Z","iopub.execute_input":"2024-10-11T05:21:39.720799Z","iopub.status.idle":"2024-10-11T05:22:10.363783Z","shell.execute_reply.started":"2024-10-11T05:21:39.720763Z","shell.execute_reply":"2024-10-11T05:22:10.362773Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Generate lyric translation","metadata":{}},{"cell_type":"code","source":"# Use the translation model\nuser_chat_template = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n\"\n\n# Input the lyric from the source of a song (\"Don't Get On My Way\" by Hiroshi Kakizaki & r-Project)\njapanese_lyric = \"けして誰にも邪魔はさせない\"\n\n# Assign the translation prompt as an input\nprompt = (\n    user_chat_template.format(prompt=japanese_lyric)\n    + \"<start_of_turn>model\\n\"\n)\n\n# Generate the lyric translation as an output\nresult = model.generate(\n    prompt,\n    device=device,\n    output_len=256\n)\n\n# Print the result\nprint(\"Generated output (Japanese): \", result)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T05:24:34.096980Z","iopub.execute_input":"2024-10-11T05:24:34.097456Z","iopub.status.idle":"2024-10-11T05:24:47.191170Z","shell.execute_reply.started":"2024-10-11T05:24:34.097415Z","shell.execute_reply":"2024-10-11T05:24:47.190237Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Generated output (Japanese):  その言葉、とても強い意志を感じます！ \n\nどんなことがあって、誰かを邪魔しないために、強い思いで行動しているのでしょうか？\n\n\n \n \n\n\n<end_of_turn>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Multilingual translations","metadata":{}},{"cell_type":"code","source":"# Use MarianMT model as a function to translate text\ndef translate_text(text, target_language):\n    \"\"\"Translates text from Japanese to the target language using MarianMT models.\"\"\"\n    \n    # Step 1: Model name based on the target language, e.g. Spanish (Español), Italian (Italiano), French (Français), English\n    model_name = f'Helsinki-NLP/opus-mt-ja-{target_language}'\n    \n    # Step 2: Tokenizer and model for translation\n    tokenizer = MarianTokenizer.from_pretrained(model_name)\n    model = MarianMTModel.from_pretrained(model_name)\n    \n    # Step 3: Input text tokenization\n    tokenized_text = tokenizer(text, return_tensors='pt', padding=True)\n    \n    # Step 4: Generate translation\n    translated = model.generate(**tokenized_text)\n    \n    # Step 5: Decode output for text translation\n    return tokenizer.decode(translated[0], skip_special_tokens=True)\n\n# Retrieve the lyric example\njapanese_lyric = \"けして誰にも邪魔はさせない\"\n\n# Perform translation to Spanish, Italian, French and English\nspanish_translation = translate_text(japanese_lyric, 'es')\nitalian_translation = translate_text(japanese_lyric, 'it')\nfrench_translation = translate_text(japanese_lyric, 'fr')\nenglish_translation = translate_text(japanese_lyric, 'en')\n\n# Print the translations\nprint(\"Japanese input: \", japanese_lyric)\nprint(\"Translation in Spanish: \", spanish_translation)\nprint(\"Translation in Italian: \", italian_translation)\nprint(\"Translation in French: \", french_translation)\nprint(\"Translation in English: \", english_translation)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T05:25:21.296933Z","iopub.execute_input":"2024-10-11T05:25:21.297344Z","iopub.status.idle":"2024-10-11T05:27:03.888752Z","shell.execute_reply.started":"2024-10-11T05:25:21.297305Z","shell.execute_reply":"2024-10-11T05:27:03.887779Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e97c7daaa70f4037b1bf060f4adfcd3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/788k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f69e9564f0e942278bb8643324d11266"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/829k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6040fd31b894fd3b01184e3a48bf8e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.58M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4207f26bd4b44ae388c847e51802a418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8c91f5d6dbf4e4a829a08315f16f62d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00b4584382ff4db5993e84fbceb242fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d77e13d88d1d47008092650f9707a923"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8039fd4be7354141909c385304138337"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/834k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39e07ffffbc147f68122c26d14d5023c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/832k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1d1421a68364479a938ce074b0a5bec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2020f508906442de859707ca7fe1e4c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fb6fa86517b411f8b6a33f933a532b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee21980462564bdd80eb69c7d42f1c85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7be0ffeea024afab477dbbc932f91f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7060fc2fdf8947a5b040d7ee8cac8e28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/788k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92570803c4ff45d1b7e9e67932dc1e07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/828k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e71904677ad4023ac10fb14243c34a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.59M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e497acb7029f4287b93bdc824ef2e9b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be93844f0d7c4f33b1196bdb95d144ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04e9ee461b2846fbbe32dc2598b0fa3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e3e23e8c2d14b8e8a827330bfd98414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b1d7fc58a841d38d791340ef48fafb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/782k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43b8b86c3fd8476d90a2cd2d44d44430"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4974e69e787148bc889b8924d63d4092"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66c912922b2345a7914eee6e6c8f13bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb823d609ccc44bc8108f3d73aaa4798"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/303M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8f706ee4071434080c5394c81db62a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5952a34eb54c40beaafaec6029b05e9d"}},"metadata":{}},{"name":"stdout","text":"Japanese input:  けして誰にも邪魔はさせない\nTranslation in Spanish:  No voy a dejar que nadie se meta en mi camino.\nTranslation in Italian:  Non permettero' a nessuno di interferire.\nTranslation in French:  Je ne laisserai personne m'en empêcher.\nTranslation in English:  I'm not letting anyone get in my way.\n","output_type":"stream"}]}]}