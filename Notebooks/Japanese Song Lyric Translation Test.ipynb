{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":116995,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":98328,"modelId":121954}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"# Install environment packages first\n!pip install -q -U immutabledict sentencepiece\n!pip install -q -U sacremoses transformers\n!git clone https://github.com/google/gemma_pytorch.git\n!mkdir /kaggle/working/gemma/\n!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/\n\n# Install JupyterLab widgets module\n!pip uninstall jupyterlab_widgets -y\n!pip install jupyterlab_widgets","metadata":{"execution":{"iopub.status.busy":"2025-01-17T07:33:18.036179Z","iopub.execute_input":"2025-01-17T07:33:18.036458Z","iopub.status.idle":"2025-01-17T07:33:47.278069Z","shell.execute_reply.started":"2025-01-17T07:33:18.036431Z","shell.execute_reply":"2025-01-17T07:33:47.276934Z"},"trusted":true},"outputs":[{"name":"stdout","text":"fatal: destination path 'gemma_pytorch' already exists and is not an empty directory.\nmkdir: cannot create directory '/kaggle/working/gemma/': File exists\nmv: cannot stat '/kaggle/working/gemma_pytorch/gemma/*': No such file or directory\nFound existing installation: jupyterlab_widgets 3.0.13\nUninstalling jupyterlab_widgets-3.0.13:\n  Successfully uninstalled jupyterlab_widgets-3.0.13\nCollecting jupyterlab_widgets\n  Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\nUsing cached jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\nInstalling collected packages: jupyterlab_widgets\nSuccessfully installed jupyterlab_widgets-3.0.13\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Import primary function libraries\nimport contextlib\nimport gemma\nimport os\nimport sys\nimport torch\nimport transformers\n\n# Import secondary function libraries\nfrom transformers import MarianMTModel, MarianTokenizer\nfrom gemma.config import GemmaConfig, get_model_config\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer","metadata":{"execution":{"iopub.status.busy":"2025-01-17T07:21:18.356738Z","iopub.execute_input":"2025-01-17T07:21:18.357135Z","iopub.status.idle":"2025-01-17T07:21:35.834445Z","shell.execute_reply.started":"2025-01-17T07:21:18.357101Z","shell.execute_reply":"2025-01-17T07:21:35.833766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define functional preferences","metadata":{}},{"cell_type":"code","source":"# Model variant and machine classification\nvariant = \"2b-v2\"\nmachine_type = \"cuda\"\nmodel_path = '/kaggle/input/gemma-2-2b-jpn-it/pytorch/gemma-2-2b-jpn-it/1/'\nweights_file = os.path.join(model_path, \"model.ckpt\")\n\n# Set the default Tensor type\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\n# Assign the model configuration along with the tokenizer\nmodel_config = get_model_config(variant)\nmodel_config.tokenizer = os.path.join(model_path, \"tokenizer.model\")\n\n# Set the device, e.g. GPU or CPU\ndevice = torch.device(machine_type)\n\n# Load the model\nwith _set_default_tensor_type(model_config.get_dtype()):\n    model = GemmaForCausalLM(model_config)\n    model.load_weights(weights_file)\n    model = model.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2025-01-17T07:34:21.719278Z","iopub.execute_input":"2025-01-17T07:34:21.719678Z","iopub.status.idle":"2025-01-17T07:34:26.918238Z","shell.execute_reply.started":"2025-01-17T07:34:21.719645Z","shell.execute_reply":"2025-01-17T07:34:26.917276Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Generate lyric translation","metadata":{}},{"cell_type":"code","source":"# Use the translation model\nuser_chat_template = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n\"\n\n# Input the lyric from the source of a song (\"Don't Get On My Way\" by Hiroshi Kakizaki & r-Project)\njapanese_lyric = \"ã‘ã—ã¦èª°ã«ã‚‚é‚ªé­”ã¯ã•ã›ãªã„\"\n\n# Assign the translation prompt as an input\nprompt = (\n    user_chat_template.format(prompt=japanese_lyric)\n    + \"<start_of_turn>model\\n\"\n)\n\n# Generate the lyric translation as an output\nresult = model.generate(\n    prompt,\n    device=device,\n    output_len=256\n)\n\n# Print the result\nprint(\"Generated output (Japanese): \", result)","metadata":{"execution":{"iopub.status.busy":"2025-01-17T07:34:39.250121Z","iopub.execute_input":"2025-01-17T07:34:39.250710Z","iopub.status.idle":"2025-01-17T07:34:50.927457Z","shell.execute_reply.started":"2025-01-17T07:34:39.250677Z","shell.execute_reply":"2025-01-17T07:34:50.926446Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Generated output (Japanese):  ãã®æ°—æŒã¡ã€ã‚ˆãåˆ†ã‹ã‚Šã¾ã™ï¼ ğŸ˜Š \n\nã€Œè‡ªåˆ†ã®ãƒšãƒ¼ã‚¹ã§ã€è‡ªåˆ†ã®ã‚„ã‚ŠãŸã„ã“ã¨ã‚’è¿½æ±‚ã—ãŸã„ã€ã¨ã„ã†æ°—æŒã¡ã£ã¦ã€ã¨ã¦ã‚‚ç´ æ™´ã‚‰ã—ã„ã§ã™ã­ã€‚ \n\nã§ã‚‚ã€æ™‚ã«ã¯å‘¨å›²ã®äººã¨å”åŠ›ã—ãŸã‚Šã€åŠ©ã‘åˆã†ã“ã¨ã‚‚å¤§åˆ‡ã§ã™ã€‚ \nå‘¨ã‚Šã®äººã«ã€Œé‚ªé­”ã€ã ã¨æ„Ÿã˜ã‚‰ã‚Œã‚‹ã‚ˆã†ãªã“ã¨ã¯ã€ä»–ã«è‰¯ã„æ–¹æ³•ã¯ãªã„ã‹ãªï¼Ÿ ğŸ˜Š\n\nã‚‚ã—ã€ä½•ã‹å›°ã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚Œã°ã€æ°—è»½ã«ç›¸è«‡ã—ã¦ãã ã•ã„ã­ï¼  \n\n\n\n\n<end_of_turn>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Multilingual translations","metadata":{}},{"cell_type":"code","source":"# Use MarianMT model as a function to translate text\ndef translate_text(text, target_language):\n    \"\"\"Translates text from Japanese to the target language using MarianMT models.\"\"\"\n    \n    # Step 1: Model name based on the target language, e.g. Spanish (EspaÃ±ol), Italian (Italiano), French (FranÃ§ais), English\n    model_name = f'Helsinki-NLP/opus-mt-ja-{target_language}'\n    \n    # Step 2: Tokenizer and model for translation\n    tokenizer = MarianTokenizer.from_pretrained(model_name)\n    model = MarianMTModel.from_pretrained(model_name)\n    \n    # Step 3: Input text tokenization\n    tokenized_text = tokenizer(text, return_tensors='pt', padding=True)\n    \n    # Step 4: Generate translation\n    translated = model.generate(**tokenized_text)\n    \n    # Step 5: Decode output for text translation\n    return tokenizer.decode(translated[0], skip_special_tokens=True)\n\n# Retrieve the lyric example\njapanese_lyric = \"ã‘ã—ã¦èª°ã«ã‚‚é‚ªé­”ã¯ã•ã›ãªã„\"\n\n# Perform translation to Spanish, Italian, French and English\nspanish_translation = translate_text(japanese_lyric, 'es')\nportuguese_translation = translate_text(japanese_lyric, 'pt')\nitalian_translation = translate_text(japanese_lyric, 'it')\nfrench_translation = translate_text(japanese_lyric, 'fr')\ndanish_translation = translate_text(japanese_lyric, 'da')\nswedish_translation = translate_text(japanese_lyric, 'sv')\nhungarian_translation = translate_text(japanese_lyric, 'hu')\nfinnish_translation = translate_text(japanese_lyric, 'fi')\ngerman_translation = translate_text(japanese_lyric, 'de')\ndutch_translation = translate_text(japanese_lyric, 'nl')\nenglish_translation = translate_text(japanese_lyric, 'en')\n\n# Print the translations\nprint(\"Japanese input: \", japanese_lyric)\nprint(\"Translation in Spanish: \", spanish_translation)\nprint(\"Translation in Portuguese: \", portuguese_translation)\nprint(\"Translation in Italian: \", italian_translation)\nprint(\"Translation in French: \", french_translation)\nprint(\"Translation in Danish: \", danish_translation)\nprint(\"Translation in Swedish: \", swedish_translation)\nprint(\"Translation in Hungarian: \", hungarian_translation)\nprint(\"Translation in Finnish: \", finnish_translation)\nprint(\"Translation in German: \", german_translation)\nprint(\"Translation in Dutch: \", dutch_translation)\nprint(\"Translation in English: \", english_translation)","metadata":{"execution":{"iopub.status.busy":"2025-01-17T07:34:57.776283Z","iopub.execute_input":"2025-01-17T07:34:57.776655Z","iopub.status.idle":"2025-01-17T07:35:22.765738Z","shell.execute_reply.started":"2025-01-17T07:34:57.776622Z","shell.execute_reply":"2025-01-17T07:35:22.764820Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Japanese input:  ã‘ã—ã¦èª°ã«ã‚‚é‚ªé­”ã¯ã•ã›ãªã„\nTranslation in Spanish:  No voy a dejar que nadie se meta en mi camino.\nTranslation in Portuguese:  NÃ£o vou deixar que ninguÃ©m se meta no meu caminho.\nTranslation in Italian:  Non permettero' a nessuno di interferire.\nTranslation in French:  Je ne laisserai personne m'en empÃªcher.\nTranslation in Danish:  Jeg vil ikke lade nogen komme i vejen for dig.\nTranslation in Swedish:  Jag tÃ¤nker inte lÃ¥ta nÃ¥gon stÃ¶ra mig.\nTranslation in Hungarian:  Senki sem Ã¡llhat az utamba.\nTranslation in Finnish:  En anna kenenkÃ¤Ã¤n hÃ¤iritÃ¤ sinua.\nTranslation in German:  Ich lasse niemanden aus dem Weg gehen.\nTranslation in Dutch:  Ik laat niemand je in de weg staan.\nTranslation in English:  I'm not letting anyone get in my way.\n","output_type":"stream"}],"execution_count":13}]}