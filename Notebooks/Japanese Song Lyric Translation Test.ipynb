{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":116995,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":98328,"modelId":121954}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"# Install environment packages first\n!pip install -q -U immutabledict sentencepiece\n!pip install -q -U sacremoses transformers\n!git clone https://github.com/google/gemma_pytorch.git\n!mkdir /kaggle/working/gemma/\n!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/\n\n# Install JupyterLab widgets module\n!pip uninstall jupyterlab_widgets -y\n!pip install jupyterlab_widgets","metadata":{"execution":{"iopub.status.busy":"2025-01-17T07:24:05.224804Z","iopub.execute_input":"2025-01-17T07:24:05.225182Z","iopub.status.idle":"2025-01-17T07:24:34.577618Z","shell.execute_reply.started":"2025-01-17T07:24:05.225146Z","shell.execute_reply":"2025-01-17T07:24:34.576679Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"fatal: destination path 'gemma_pytorch' already exists and is not an empty directory.\nmkdir: cannot create directory '/kaggle/working/gemma/': File exists\nmv: cannot stat '/kaggle/working/gemma_pytorch/gemma/*': No such file or directory\nFound existing installation: jupyterlab_widgets 3.0.13\nUninstalling jupyterlab_widgets-3.0.13:\n  Successfully uninstalled jupyterlab_widgets-3.0.13\nCollecting jupyterlab_widgets\n  Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\nUsing cached jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\nInstalling collected packages: jupyterlab_widgets\nSuccessfully installed jupyterlab_widgets-3.0.13\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Import primary function libraries\nimport contextlib\nimport gemma\nimport os\nimport sys\nimport torch\nimport transformers\n\n# Import secondary function libraries\nfrom transformers import MarianMTModel, MarianTokenizer\nfrom gemma.config import GemmaConfig, get_model_config\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer","metadata":{"execution":{"iopub.status.busy":"2025-01-17T07:21:18.356738Z","iopub.execute_input":"2025-01-17T07:21:18.357135Z","iopub.status.idle":"2025-01-17T07:21:35.834445Z","shell.execute_reply.started":"2025-01-17T07:21:18.357101Z","shell.execute_reply":"2025-01-17T07:21:35.833766Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Define functional preferences","metadata":{}},{"cell_type":"code","source":"# Model variant and machine classification\nvariant = \"2b-v2\"\nmachine_type = \"cuda\"\nmodel_path = '/kaggle/input/gemma-2-2b-jpn-it/pytorch/gemma-2-2b-jpn-it/1/'\nweights_file = os.path.join(model_path, \"model.ckpt\")\n\n# Set the default Tensor type\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\n# Assign the model configuration along with the tokenizer\nmodel_config = get_model_config(variant)\nmodel_config.tokenizer = os.path.join(model_path, \"tokenizer.model\")\n\n# Set the device, e.g. GPU or CPU\ndevice = torch.device(machine_type)\n\n# Load the model\nwith _set_default_tensor_type(model_config.get_dtype()):\n    model = GemmaForCausalLM(model_config)\n    model.load_weights(weights_file)\n    model = model.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2025-01-17T07:21:41.789683Z","iopub.execute_input":"2025-01-17T07:21:41.790351Z","iopub.status.idle":"2025-01-17T07:22:10.449925Z","shell.execute_reply.started":"2025-01-17T07:21:41.790314Z","shell.execute_reply":"2025-01-17T07:22:10.449168Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Generate lyric translation","metadata":{}},{"cell_type":"code","source":"# Use the translation model\nuser_chat_template = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n\"\n\n# Input the lyric from the source of a song (\"Don't Get On My Way\" by Hiroshi Kakizaki & r-Project)\njapanese_lyric = \"けして誰にも邪魔はさせない\"\n\n# Assign the translation prompt as an input\nprompt = (\n    user_chat_template.format(prompt=japanese_lyric)\n    + \"<start_of_turn>model\\n\"\n)\n\n# Generate the lyric translation as an output\nresult = model.generate(\n    prompt,\n    device=device,\n    output_len=256\n)\n\n# Print the result\nprint(\"Generated output (Japanese): \", result)","metadata":{"execution":{"iopub.status.busy":"2025-01-17T07:22:23.388700Z","iopub.execute_input":"2025-01-17T07:22:23.389052Z","iopub.status.idle":"2025-01-17T07:22:35.651422Z","shell.execute_reply.started":"2025-01-17T07:22:23.389022Z","shell.execute_reply":"2025-01-17T07:22:35.650488Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Generated output (Japanese):  その気持ちはよく分かります！ \nどんなことでも、自分のペースで、自分の道を歩むことが大切ですよね。 \n\n\n \n<end_of_turn>\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Multilingual translations","metadata":{}},{"cell_type":"code","source":"# Use MarianMT model as a function to translate text\ndef translate_text(text, target_language):\n    \"\"\"Translates text from Japanese to the target language using MarianMT models.\"\"\"\n    \n    # Step 1: Model name based on the target language, e.g. Spanish (Español), Italian (Italiano), French (Français), English\n    model_name = f'Helsinki-NLP/opus-mt-ja-{target_language}'\n    \n    # Step 2: Tokenizer and model for translation\n    tokenizer = MarianTokenizer.from_pretrained(model_name)\n    model = MarianMTModel.from_pretrained(model_name)\n    \n    # Step 3: Input text tokenization\n    tokenized_text = tokenizer(text, return_tensors='pt', padding=True)\n    \n    # Step 4: Generate translation\n    translated = model.generate(**tokenized_text)\n    \n    # Step 5: Decode output for text translation\n    return tokenizer.decode(translated[0], skip_special_tokens=True)\n\n# Retrieve the lyric example\njapanese_lyric = \"けして誰にも邪魔はさせない\"\n\n# Perform translation to Spanish, Italian, French and English\nspanish_translation = translate_text(japanese_lyric, 'es')\nportuguese_translation = translate_text(japanese_lyric, 'pt')\nitalian_translation = translate_text(japanese_lyric, 'it')\nfrench_translation = translate_text(japanese_lyric, 'fr')\ndanish_translation = translate_text(japanese_lyric, 'da')\nswedish_translation = translate_text(japanese_lyric, 'sv')\nhungarian_translation = translate_text(japanese_lyric, 'hu')\nfinnish_translation = translate_text(japanese_lyric, 'fi')\ngerman_translation = translate_text(japanese_lyric, 'de')\ndutch_translation = translate_text(japanese_lyric, 'nl')\nenglish_translation = translate_text(japanese_lyric, 'en')\n\n# Print the translations\nprint(\"Japanese input: \", japanese_lyric)\nprint(\"Translation in Spanish: \", spanish_translation)\nprint(\"Translation in Portuguese: \", portuguese_translation)\nprint(\"Translation in Italian: \", italian_translation)\nprint(\"Translation in French: \", french_translation)\nprint(\"Translation in Danish: \", danish_translation)\nprint(\"Translation in Swedish: \", swedish_translation)\nprint(\"Translation in Hungarian: \", hungarian_translation)\nprint(\"Translation in Finnish: \", finnish_translation)\nprint(\"Translation in German: \", german_translation)\nprint(\"Translation in Dutch: \", dutch_translation)\nprint(\"Translation in English: \", english_translation)","metadata":{"execution":{"iopub.status.busy":"2025-01-17T07:24:45.536862Z","iopub.execute_input":"2025-01-17T07:24:45.537239Z","iopub.status.idle":"2025-01-17T07:25:10.026740Z","shell.execute_reply.started":"2025-01-17T07:24:45.537208Z","shell.execute_reply":"2025-01-17T07:25:10.025828Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Japanese input:  けして誰にも邪魔はさせない\nTranslation in Spanish:  No voy a dejar que nadie se meta en mi camino.\nTranslation in Portuguese:  Não vou deixar que ninguém se meta no meu caminho.\nTranslation in Italian:  Non permettero' a nessuno di interferire.\nTranslation in French:  Je ne laisserai personne m'en empêcher.\nTranslation in Danish:  Jeg vil ikke lade nogen komme i vejen for dig.\nTranslation in Swedish:  Jag tänker inte låta någon störa mig.\nTranslation in Hungarian:  Senki sem állhat az utamba.\nTranslation in Finnish:  En anna kenenkään häiritä sinua.\nTranslation in German:  Ich lasse niemanden aus dem Weg gehen.\nTranslation in Dutch:  Ik laat niemand je in de weg staan.\nTranslation in English:  I'm not letting anyone get in my way.\n","output_type":"stream"}],"execution_count":7}]}